JEPA - Video Classification using Masked Latent Prediction
This repository contains an implementation of the Joint Embedding Predictive Architecture (JEPA) applied to video classification tasks. JEPA is a masked prediction model that learns spatiotemporal representations by predicting the latent representations of masked video patches.

ğŸ“Œ Features
Upload and classify short videos using a pretrained JEPA model

Uses Gradio interface for interactive video uploads

Built-in webcam support (optional)

Lightweight and runs on Colab or local GPU

ğŸ§  Model Architecture
JEPA leverages self-supervised learning by masking portions of the input and training a transformer-based encoder to predict the latent features of the masked regions. This approach allows the model to learn general representations of video dynamics without requiring explicit frame-level annotations.

ğŸš€ Getting Started
1. Clone the repository
bash
Copy
Edit
git clone https://github.com/your-username/jepa-video-classification.git
cd jepa-video-classification
2. Install Dependencies
Use the following command to install required libraries:

bash
Copy
Edit
pip install -r requirements.txt
If youâ€™re using Google Colab, the required libraries should install automatically in the notebook.

3. Run the Notebook
Open Jepa.ipynb in Jupyter or Colab:

Open in Google Colab

Follow the cells to run the interface.

4. Run Locally with Gradio (optional)
To run the interface locally:

bash
Copy
Edit
python app.py
ğŸ§ª Example Use

The model will output the predicted action or category

ğŸ“ Project Structure
bash
Copy
Edit
â”œâ”€â”€ Jepa.ipynb           # Main notebook
â”œâ”€â”€ app.py               # Optional Gradio script (if separated)
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
ğŸ”§ Requirements
Python 3.8+

torch, transformers, gradio, opencv-python

âœï¸ Author
Muhammad Amin
Email: muhammadamin0334@gmail.com

ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.
